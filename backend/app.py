#!/usr/bin/env python3
"""
Neural Canvas Backend API (Flask)
==================================
Flask backend for CNN image classification and LLM explanations
"""

import os
from pathlib import Path

# Get project root directory (works on both Windows and Linux)
BACKEND_DIR = Path(__file__).resolve().parent
FRONTEND_DIR = BACKEND_DIR.parent
PROJECT_ROOT = FRONTEND_DIR.parent

# Set HF cache - use project-relative or server path
if os.path.exists('/cs/student/projects1/2023/muhamaaz/datasets'):
    os.environ['HF_HOME'] = '/cs/student/projects1/2023/muhamaaz/datasets'
    os.environ['TRANSFORMERS_CACHE'] = '/cs/student/projects1/2023/muhamaaz/datasets'
else:
    # Use default cache on local machine
    pass

from flask import Flask, request, jsonify
from flask_cors import CORS
from typing import Optional, List
import torch
import torch.nn.functional as F
from PIL import Image
import torchvision.transforms as transforms
import io
import sys
import re
import numpy as np
import requests
from groq import Groq

# Add paths (cross-platform)
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / 'llm'))
sys.path.insert(0, str(PROJECT_ROOT / 'cnn_models'))

from transformers import AutoTokenizer, AutoModelForCausalLM
from llm.models.art_expert_model import create_art_expert_model
from cnn_models.model import build_model
from cnn_models.config import Config

app = Flask(__name__)
CORS(app)  # Enable CORS for Next.js frontend

# Global models (loaded on startup)
cnn_model_scratch = None  # CNN from scratch
cnn_model_finetuned = None  # Fine-tuned ResNet50
llm_model1 = None
llm_model2 = None
llm_model3_client = None  # Groq API client
tokenizer = None
device = None
artist_names = None
style_names = None
genre_names = None

# Groq API configuration
GROQ_API_KEY = os.getenv('GROQ_API_KEY', '')  # Set via environment variable
GROQ_MODEL = "llama-3.1-8b-instant"  # Updated from decommissioned llama-3.2-1b-preview


def load_models():
    """Load all models on startup"""
    global cnn_model_scratch, cnn_model_finetuned, llm_model1, llm_model2, llm_model3_client, tokenizer, device, artist_names, style_names, genre_names
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Loading models on {device}...")
    
    # Get class names
    from datasets import load_dataset
    dataset = load_dataset("huggan/wikiart", split="train", streaming=True)
    artist_names = dataset.features['artist'].names
    style_names = dataset.features['style'].names
    genre_names = dataset.features['genre'].names
    
    num_classes = {
        'artist': len(artist_names),
        'style': len(style_names),
        'genre': len(genre_names)
    }
    
    # Load CNN from scratch (Custom CNN)
    print("Loading CNN from scratch (Custom CNN)...")
    config = Config()
    cnn_model_scratch = build_model(config, num_classes).to(device)
    
    # Try multiple possible checkpoint locations for scratch model (cross-platform)
    scratch_checkpoints = [
        str(PROJECT_ROOT / 'cnn_models' / 'checkpoints' / 'best_models' / 'best_custom_cnn.pt'),
        str(PROJECT_ROOT / 'cnn_models' / 'checkpoints' / 'best_multitask_macro0.6421.pt'),
        "/cs/student/projects1/2023/muhamaaz/neural-canvas/cnn_models/checkpoints/best_models/best_custom_cnn.pt",
        "/cs/student/projects1/2023/muhamaaz/neural-canvas/cnn_models/checkpoints/best_multitask_macro0.6421.pt"
    ]
    
    cnn_checkpoint_scratch = None
    for ckpt_path in scratch_checkpoints:
        if os.path.exists(ckpt_path):
            cnn_checkpoint_scratch = ckpt_path
            break
    
    if cnn_checkpoint_scratch:
        ckpt = torch.load(cnn_checkpoint_scratch, map_location=device, weights_only=False)
        # Handle different checkpoint formats
        if 'model' in ckpt:
            cnn_model_scratch.load_state_dict(ckpt['model'])
        elif 'model_state_dict' in ckpt:
            cnn_model_scratch.load_state_dict(ckpt['model_state_dict'])
        else:
            cnn_model_scratch.load_state_dict(ckpt)
        macro_acc = ckpt.get('macro_acc', ckpt.get('val_acc', 0))
        print(f"✓ CNN from scratch loaded from {cnn_checkpoint_scratch} (macro acc: {macro_acc:.2%})")
    else:
        print("⚠ CNN scratch checkpoint not found")
    cnn_model_scratch.eval()
    
    # Load fine-tuned model (TIMM-based - ConvNeXt-Tiny)
    print("Loading fine-tuned model (TIMM ConvNeXt-Tiny)...")
    cnn_model_finetuned = None
    try:
        # Import from model_timm.py (not train_timm.py)
        from model_timm import TimmMultiHead
        
        # Create TIMM model with ConvNeXt-Tiny backbone
        cnn_model_finetuned = TimmMultiHead(num_classes=num_classes, model_name="convnext_tiny").to(device)
        
        # Find TIMM checkpoint - check multiple possible locations (cross-platform)
        import glob
        finetuned_checkpoints = []
        
        # Priority: best_timm.pt in best_models folder
        cnn_models_path = PROJECT_ROOT / 'cnn_models'
        
        # Check cross-platform paths first
        finetuned_checkpoints.extend(glob.glob(str(cnn_models_path / 'checkpoints' / 'best_models' / 'best_timm.pt')))
        finetuned_checkpoints.extend(glob.glob(str(cnn_models_path / 'checkpoints' / 'best_models' / '*timm*.pt')))
        finetuned_checkpoints.extend(glob.glob(str(cnn_models_path / 'checkpoints' / '*timm*.pt')))
        finetuned_checkpoints.extend(glob.glob(str(cnn_models_path / 'checkpoints' / '*convnext*.pt')))
        
        # Fallback to server paths
        finetuned_checkpoints.extend(glob.glob("/cs/student/projects1/2023/muhamaaz/neural-canvas/cnn_models/checkpoints/best_models/best_timm.pt"))
        finetuned_checkpoints.extend(glob.glob("/cs/student/projects1/2023/muhamaaz/neural-canvas/cnn_models/checkpoints/*timm*.pt"))
        finetuned_checkpoints.extend(glob.glob("/cs/student/projects1/2023/muhamaaz/neural-canvas/cnn_models/checkpoints/*convnext*.pt"))
        
        if finetuned_checkpoints:
            # Prefer TIMM-specific checkpoints, then checkpoints with macro in name
            timm_checkpoints = [c for c in finetuned_checkpoints if 'timm' in c.lower() or 'convnext' in c.lower()]
            if timm_checkpoints:
                # Sort by macro accuracy if in filename, otherwise by modification time
                best_ckpt = max(timm_checkpoints, key=lambda x: (
                    float(x.split('macro')[1].split('.pt')[0]) if 'macro' in x.lower() else 0,
                    os.path.getmtime(x)
                ))
            else:
                # Use checkpoint with highest macro accuracy in name
                macro_checkpoints = [c for c in finetuned_checkpoints if 'macro' in c.lower()]
                if macro_checkpoints:
                    best_ckpt = max(macro_checkpoints, key=lambda x: float(x.split('macro')[1].split('.pt')[0]) if 'macro' in x else 0)
                else:
                    best_ckpt = max(finetuned_checkpoints, key=lambda x: os.path.getmtime(x))
            
            print(f"Loading TIMM checkpoint: {best_ckpt}")
            ckpt = torch.load(best_ckpt, map_location=device, weights_only=False)
            
            # Handle different checkpoint formats
            if 'model' in ckpt:
                cnn_model_finetuned.load_state_dict(ckpt['model'])
            elif 'model_state_dict' in ckpt:
                cnn_model_finetuned.load_state_dict(ckpt['model_state_dict'])
            else:
                cnn_model_finetuned.load_state_dict(ckpt)
            
            macro_acc = ckpt.get('macro_acc', ckpt.get('val_acc', ckpt.get('best_macro_acc', 0)))
            print(f"✓ Fine-tuned TIMM model loaded (macro acc: {macro_acc:.2%})")
            cnn_model_finetuned.eval()
        else:
            print("⚠ TIMM checkpoint not found - fine-tuned model will not be available")
            print("   Expected locations:")
            print("   - /cnn_models/checkpoints/*timm*.pt")
            print("   - /cnn_models/checkpoints/*convnext*.pt")
            print("   - /cnn_models/checkpoints/best_models/*timm*.pt")
            cnn_model_finetuned = None
    except Exception as e:
        print(f"⚠ Could not load fine-tuned CNN: {e}")
        import traceback
        traceback.print_exc()
        cnn_model_finetuned = None
    
    # Load tokenizer
    print("Loading tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    tokenizer.pad_token = tokenizer.eos_token
    
    # Load LLM Model 1 (From Scratch)
    print("Loading LLM Model 1 (From Scratch)...")
    llm1_paths = [
        str(PROJECT_ROOT / 'checkpoints' / 'cnn_explainer_from_scratch' / 'best_model.pt'),
        "/cs/student/projects1/2023/muhamaaz/checkpoints/cnn_explainer_from_scratch/best_model.pt"
    ]
    llm1_path = next((p for p in llm1_paths if os.path.exists(p)), None)
    if llm1_path:
        checkpoint = torch.load(llm1_path, map_location=device, weights_only=False)
        llm_model1 = create_art_expert_model(tokenizer.vocab_size, "base").to(device)
        llm_model1.load_state_dict(checkpoint['model_state_dict'])
        llm_model1.eval()
        print(f"✓ Model 1 loaded from {llm1_path}")
    else:
        print("⚠ LLM Model 1 not found")
        llm_model1 = None
    
    # Load LLM Model 2 (Fine-tuned GPT-2 Medium - 355M params)
    print("Loading LLM Model 2 (Fine-tuned GPT-2 Medium - 355M params)...")
    model2_paths = [
        str(PROJECT_ROOT / 'checkpoints' / 'model2_cnn_explainer_gpt2medium' / 'best_model_hf'),
        str(PROJECT_ROOT / 'checkpoints' / 'cnn_explainer_finetuned' / 'best_model_hf'),
        "/cs/student/projects1/2023/muhamaaz/checkpoints/model2_cnn_explainer_gpt2medium/best_model_hf",
        "/cs/student/projects1/2023/muhamaaz/checkpoints/cnn_explainer_finetuned/best_model_hf"
    ]
    model2_path = next((p for p in model2_paths if os.path.exists(p)), None)
    
    if os.path.exists(model2_path):
        llm_model2 = AutoModelForCausalLM.from_pretrained(model2_path).to(device)
        llm_model2.eval()
        print("✓ Model 2 loaded")
    else:
        print("⚠ Model 2 not found, using Model 1 only")
        llm_model2 = None

    # Load LLM Model 3 (Groq API - Llama 3.2 1B)
    print("Loading LLM Model 3 (Groq API - Llama 3.2 1B)...")
    try:
        llm_model3_client = Groq(api_key=GROQ_API_KEY)
        print("✓ Model 3 (Groq) loaded")
    except Exception as e:
        print(f"⚠ Model 3 (Groq) failed to load: {e}")
        llm_model3_client = None

    print("✅ All models loaded!")


def preprocess_image(image: Image.Image) -> torch.Tensor:
    """Preprocess image for CNN"""
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])
    return transform(image.convert('RGB')).unsqueeze(0)


def predict_cnn(image: Image.Image, model_type: str = "scratch", top_k: int = 3) -> dict:
    """Run CNN prediction on image
    
    Args:
        image: PIL Image
        model_type: "scratch" or "finetuned"
        top_k: Number of top predictions to return
    """
    global cnn_model_scratch, cnn_model_finetuned, device, artist_names, style_names, genre_names
    
    model = cnn_model_scratch if model_type == "scratch" else cnn_model_finetuned
    if model is None:
        raise ValueError(f"CNN model {model_type} not loaded")
    
    img_tensor = preprocess_image(image).to(device)
    
    with torch.no_grad():
        logits = model(img_tensor)
    
    predictions = {}
    top_predictions = {}
    
    for task in ['artist', 'style', 'genre']:
        probs = F.softmax(logits[task], dim=1)
        
        # Get top-k predictions
        top_confs, top_idxs = torch.topk(probs, min(top_k, probs.size(1)), dim=1)
        
        if task == 'artist':
            names = artist_names
        elif task == 'style':
            names = style_names
        else:
            names = genre_names
        
        # Build top-k list
        top_list = []
        for i in range(top_confs.size(1)):
            name = names[top_idxs[0, i].item()]
            name = name.replace('-', ' ').replace('_', ' ').title()
            conf = top_confs[0, i].item()
            top_list.append({'label': name, 'confidence': int(conf * 100)})
        
        top_predictions[task] = top_list
        
        # Keep top-1 for backward compatibility
        predictions[task] = {
            'name': top_list[0]['label'],
            'confidence': top_list[0]['confidence'] / 100.0
        }
    
    return {
        'artist': predictions['artist']['name'],
        'artist_confidence': predictions['artist']['confidence'],
        'style': predictions['style']['name'],
        'style_confidence': predictions['style']['confidence'],
        'genre': predictions['genre']['name'],
        'genre_confidence': predictions['genre']['confidence'],
        # New: top-k predictions for frontend
        'artist_top': top_predictions['artist'],
        'style_top': top_predictions['style'],
        'genre_top': top_predictions['genre']
    }


def generate_explanation(prediction: dict, model_num: int = 1, max_tokens: int = 150) -> str:
    """Generate LLM explanation for CNN prediction"""
    global llm_model1, llm_model2, llm_model3_client, tokenizer, device

    # Handle Model 3 (Groq API)
    if model_num == 3:
        if llm_model3_client is None:
            return "Model 3 (Groq) not available"

        prompt = f"""You are an art expert. Explain this artwork classification in 2-3 clear sentences.

Artist: {prediction['artist']}
Style: {prediction['style']}
Genre: {prediction['genre']}

Provide a concise art historical analysis explaining why this classification makes sense based on the artist's known style and the characteristics of {prediction['style']} art."""

        try:
            completion = llm_model3_client.chat.completions.create(
                model=GROQ_MODEL,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=0.7,
                top_p=0.9
            )
            explanation = completion.choices[0].message.content.strip()
            print(f"\n[MODEL 3 - GROQ] OUTPUT: {explanation[:200]}...\n")

            # Basic cleanup
            explanation = re.sub(r'^(Assistant|Response):\s*', '', explanation, flags=re.IGNORECASE)
            return explanation
        except Exception as e:
            print(f"[MODEL 3 - GROQ] Error: {e}")
            return f"{prediction['artist']} is renowned for {prediction['style']} style artwork. This {prediction['genre']} piece exemplifies the characteristic techniques and visual elements of the {prediction['style']} movement."

    model = llm_model1 if model_num == 1 else llm_model2
    if model is None:
        return "Model not available"
    
    # Use a much more explicit, task-specific prompt to prevent the model
    # from generating unrelated conversational text or just repeating percentages
    # For Model 1, use a simpler prompt without percentages to avoid it outputting just numbers
    if model_num == 1:
        prompt = f"""As an art expert, explain this artwork classification in 2-3 clear sentences.

Classification: {prediction['artist']} painted in {prediction['style']} style, {prediction['genre']} genre.

Expert analysis:"""
    else:
        prompt = f"""You are an art historian writing a catalog entry. Explain this artwork's classification.

Artist: {prediction['artist']}
Style: {prediction['style']}
Genre: {prediction['genre']}

Write 2-3 sentences explaining why this classification is appropriate based on the artist's known style and the characteristics of {prediction['style']} art.

Analysis:"""

    tokens = tokenizer(prompt, return_tensors='pt')['input_ids'].to(device)

    # Define stop sequences to prevent conversational markers
    stop_words = ["Human:", "Assistant:", "User:", "\n\nHuman", "\n\nAssistant", "\n\nUser"]

    # Use more controlled generation to prevent hallucination
    with torch.no_grad():
        if model_num == 1:
            # Model 1 uses custom generate method - check if it has the method
            if hasattr(model, 'generate') and callable(getattr(model, 'generate')):
                # Custom model's generate method
                output = model.generate(
                    tokens,
                    max_new_tokens=max_tokens,
                    temperature=0.8,  # Higher temperature for more diverse output
                    top_k=40,
                    top_p=0.92
                )
            else:
                # Fallback: use standard generation with better parameters
                output = model.generate(
                    tokens,
                    max_new_tokens=max_tokens,
                    temperature=0.8,
                    do_sample=True,
                    top_k=40,
                    top_p=0.92,
                    repetition_penalty=1.15,
                    no_repeat_ngram_size=3,
                    pad_token_id=tokenizer.eos_token_id if tokenizer.pad_token_id is None else tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id
                )
        else:
            # Model 2 with better parameters
            output = model.generate(
                tokens,
                max_new_tokens=max_tokens,
                temperature=0.75,  # Balanced temperature
                do_sample=True,
                top_k=40,
                top_p=0.92,
                repetition_penalty=1.2,
                no_repeat_ngram_size=3,
                pad_token_id=tokenizer.eos_token_id if tokenizer.pad_token_id is None else tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )
    
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    explanation = response[len(prompt):].strip()

    # Log raw model output for debugging
    print(f"\n[MODEL {model_num}] RAW OUTPUT: {explanation[:200]}...\n")

    # CRITICAL: Early detection of bad outputs - immediately use fallback
    # Check for percentage-only or number-heavy outputs
    word_count = len(explanation.split())
    number_count = len(re.findall(r'\d+%?', explanation))
    has_percentage = re.search(r'\d+%', explanation) is not None

    # REDUCED STRICTNESS: If output starts with numbers/percentages or is VERY number-heavy (>40%), use fallback
    if (word_count > 0 and number_count / max(word_count, 1) > 0.40) or \
       re.match(r'^[\s\d%:.,-]+$', explanation[:50] if len(explanation) > 50 else explanation):
        print(f"[FALLBACK] Triggered by number-heavy output (numbers: {number_count}/{word_count})")
        explanation = f"{prediction['artist']} is renowned for {prediction['style']} style artwork. This {prediction['genre']} piece exemplifies the characteristic techniques and visual elements of the {prediction['style']} movement, which the artist masterfully employed throughout their career."
    
    # Aggressively remove conversational markers and clean up
    # Step 1: Remove common conversational prefixes at the start
    conversational_prefixes = [
        r'^(Assistant|User|Human|Expert|Response):\s*',
        r'^(assistant|user|human|expert|response):\s*',
        r'^\s*[-*]\s*(Assistant|User|Human|Expert|Response):\s*'
    ]
    for pattern in conversational_prefixes:
        explanation = re.sub(pattern, '', explanation, flags=re.IGNORECASE | re.MULTILINE)

    # Step 2: Remove inline conversational markers
    explanation = re.sub(r'\b(Assistant|User|Human|Expert):\s*', '', explanation, flags=re.IGNORECASE)

    # Step 3: Truncate at first conversational marker if it appears mid-text
    conv_marker_match = re.search(r'\n\s*(Assistant|User|Human|Expert):', explanation, flags=re.IGNORECASE)
    if conv_marker_match:
        explanation = explanation[:conv_marker_match.start()].strip()

    # Step 4: Remove "respond" or similar instruction prefixes
    explanation = re.sub(r'^(respond|Respond|RESPOND|answer|Answer)[:\s]+', '', explanation, flags=re.IGNORECASE)
    
    # Step 5: Clean up leading punctuation and whitespace
    explanation = re.sub(r'^[:,\-\s]+', '', explanation).strip()

    # Step 6: Detect and remove unrelated conversational content
    # Truncate if we see question patterns that suggest conversational drift
    unrelated_patterns = [
        r'(How can I|Why do you|Did you|Then why)',
        r'(convenience store|steal|rob|hack)',
    ]
    for pattern in unrelated_patterns:
        match = re.search(pattern, explanation, re.IGNORECASE)
        if match:
            explanation = explanation[:match.start()].strip()
            break

    # Step 7: Validate output quality - check for art-related content
    art_keywords = ['artist', 'style', 'art', 'painting', 'artwork', 'movement', 'genre',
                    'technique', 'brush', 'color', 'composition', 'work', 'piece', 'created',
                    'known', 'canvas', 'paint', 'masterpiece', 'aesthetic', 'visual', 'expression',
                    'image', 'scene', 'depicted', 'painted', 'characteristic', 'renowned']

    # REDUCED STRICTNESS: Only use fallback if VERY short (<15 chars) OR completely lacks any art keywords
    has_art_keywords = any(keyword in explanation.lower() for keyword in art_keywords)
    if len(explanation) < 15:
        print(f"[FALLBACK] Output too short ({len(explanation)} chars): {explanation}")
        return f"{prediction['artist']} is renowned for {prediction['style']} style artwork. This {prediction['genre']} piece exemplifies the characteristic techniques and visual elements of the {prediction['style']} movement, which the artist masterfully employed throughout their career."

    # Even without art keywords, allow output if it's substantive (>50 chars)
    if not has_art_keywords and len(explanation) < 50:
        print(f"[FALLBACK] No art keywords and short output: {explanation}")
        return f"{prediction['artist']} is renowned for {prediction['style']} style artwork. This {prediction['genre']} piece exemplifies the characteristic techniques and visual elements of the {prediction['style']} movement, which the artist masterfully employed throughout their career."
    
    # Get complete sentences
    for end in ['. ', '! ', '? ']:
        last_idx = explanation.rfind(end)
        if last_idx > 50:
            return explanation[:last_idx+1].strip()
    
    return explanation.strip()


# Load models when app starts
load_models()


@app.route('/')
def root():
    return jsonify({
        "message": "Neural Canvas API",
        "version": "1.0.0",
        "description": "Deep Learning pipeline for art analysis: CNN (WikiArt) → LLM explanations (Custom 35-56M, GPT-2 Medium 355M, Groq Llama 3.2 1B)",
        "endpoints": {
            "/api/health": "GET - Health check & model status",
            "/api/analyze-image": "POST - Analyze artwork with CNN (JSON: imageUrl or multipart: file)",
            "/api/generate-llm": "POST - Generate LLM explanation (JSON: model, predictions)",
            "/api/generate-text": "POST - Generate text from LLM (JSON: prompt, model)",
            "/api/classify": "POST - Classify image with CNN (multipart: file)",
            "/api/classify-both": "POST - Classify with both CNNs (multipart: file)",
            "/api/explain": "POST - Get LLM explanation for predictions (JSON: artist, style, genre)",
            "/api/full": "POST - Full pipeline: CNN + LLM (multipart: file)"
        },
        "models": {
            "cnn": ["custom (scratch)", "fine-tuned ResNet50"],
            "llm": ["custom (35-56M)", "GPT-2 Medium (355M)", "Groq Llama 3.2 (1B)"]
        }
    })


@app.route('/api/health', methods=['GET'])
def health():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "models_loaded": {
            "cnn_scratch": cnn_model_scratch is not None,
            "cnn_finetuned": cnn_model_finetuned is not None,
            "llm_model1": llm_model1 is not None,
            "llm_model2": llm_model2 is not None,
            "llm_model3_groq": llm_model3_client is not None
        },
        "device": str(device)
    })


@app.route('/api/classify', methods=['POST'])
def classify_image():
    """
    Classify artwork image using CNN
    
    Expects: multipart/form-data with 'file' and optional 'model_type' (scratch/finetuned)
    Returns: Artist, Style, Genre with confidence scores
    """
    if 'file' not in request.files:
        return jsonify({"error": "No file provided"}), 400
    
    model_type = request.form.get('model_type', 'scratch')
    
    if model_type == "scratch" and cnn_model_scratch is None:
        return jsonify({"error": "CNN from scratch not loaded"}), 503
    if model_type == "finetuned" and cnn_model_finetuned is None:
        return jsonify({"error": "Fine-tuned CNN not loaded"}), 503
    
    try:
        # Read image
        file = request.files['file']
        image = Image.open(io.BytesIO(file.read())).convert('RGB')
        
        # Predict
        prediction = predict_cnn(image, model_type)
        
        return jsonify(prediction)
        
    except Exception as e:
        return jsonify({"error": f"Error processing image: {str(e)}"}), 500


@app.route('/api/generate', methods=['POST'])
def generate_text():
    """
    Generate text from LLM models given a prompt
    
    Expects: JSON with 'prompt', optional 'model' (model1/model2/both), optional 'max_tokens'
    """
    if llm_model1 is None:
        return jsonify({"error": "LLM models not loaded"}), 503
    
    data = request.json
    prompt = data.get('prompt', '')
    model = data.get('model', 'both')
    max_tokens = data.get('max_tokens', 150)
    
    explanations = []
    
    if model in ["model1", "both"]:
        tokens = tokenizer(prompt, return_tensors='pt')['input_ids'].to(device)
        with torch.no_grad():
            output = llm_model1.generate(tokens, max_new_tokens=max_tokens, temperature=0.7)
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        explanation = response[len(prompt):].strip()
        explanations.append({"model": "model1", "explanation": explanation})
    
    if model in ["model2", "both"] and llm_model2 is not None:
        tokens = tokenizer(prompt, return_tensors='pt')['input_ids'].to(device)
        with torch.no_grad():
            output = llm_model2.generate(
                tokens, max_new_tokens=max_tokens, temperature=0.7,
                do_sample=True, pad_token_id=tokenizer.eos_token_id
            )
        response = tokenizer.decode(output[0], skip_special_tokens=True)
        explanation = response[len(prompt):].strip()
        explanations.append({"model": "model2", "explanation": explanation})
    
    return jsonify(explanations)


@app.route('/api/explain', methods=['POST'])
def explain_classification():
    """
    Generate LLM explanation for CNN predictions
    
    Expects: JSON with artist, style, genre, artist_confidence, style_confidence, genre_confidence, optional model
    """
    if llm_model1 is None:
        return jsonify({"error": "LLM models not loaded"}), 503
    
    data = request.json
    prediction = {
        'artist': data.get('artist', ''),
        'artist_confidence': data.get('artist_confidence', 0.0),
        'style': data.get('style', ''),
        'style_confidence': data.get('style_confidence', 0.0),
        'genre': data.get('genre', ''),
        'genre_confidence': data.get('genre_confidence', 0.0)
    }
    
    model = data.get('model', 'both')
    explanations = []

    if model in ["model1", "both", "all"]:
        exp1 = generate_explanation(prediction, model_num=1)
        explanations.append({"model": "model1", "explanation": exp1})

    if model in ["model2", "both", "all"] and llm_model2 is not None:
        exp2 = generate_explanation(prediction, model_num=2)
        explanations.append({"model": "model2", "explanation": exp2})

    if model in ["model3", "all"] and llm_model3_client is not None:
        exp3 = generate_explanation(prediction, model_num=3)
        explanations.append({"model": "model3", "explanation": exp3})

    return jsonify(explanations)


@app.route('/api/classify-both', methods=['POST'])
def classify_both_cnns():
    """
    Classify with BOTH CNN models for comparison
    
    Expects: multipart/form-data with 'file'
    Returns: Predictions from both models
    """
    if 'file' not in request.files:
        return jsonify({"error": "No file provided"}), 400
    
    try:
        file = request.files['file']
        image = Image.open(io.BytesIO(file.read())).convert('RGB')
        
        scratch_pred = predict_cnn(image, "scratch")
        finetuned_pred = predict_cnn(image, "finetuned") if cnn_model_finetuned else None
        
        return jsonify({
            "scratch": scratch_pred,
            "finetuned": finetuned_pred
        })
    except Exception as e:
        return jsonify({"error": f"Error: {str(e)}"}), 500


@app.route('/api/full', methods=['POST'])
def full_pipeline():
    """
    Full pipeline: Image → CNN → LLM Explanation
    
    Expects: multipart/form-data with 'file' and optional 'cnn_model' (scratch/finetuned)
    Returns: CNN predictions + LLM explanations from both models
    """
    if 'file' not in request.files:
        return jsonify({"error": "No file provided"}), 400
    
    cnn_model = request.form.get('cnn_model', 'scratch')
    
    if (cnn_model == "scratch" and cnn_model_scratch is None) or \
       (cnn_model == "finetuned" and cnn_model_finetuned is None):
        return jsonify({"error": "CNN model not loaded"}), 503
    if llm_model1 is None:
        return jsonify({"error": "LLM models not loaded"}), 503
    
    try:
        # CNN Classification
        file = request.files['file']
        image = Image.open(io.BytesIO(file.read())).convert('RGB')
        prediction = predict_cnn(image, cnn_model)
        
        # LLM Explanations
        explanations = []
        
        exp1 = generate_explanation(prediction, model_num=1)
        explanations.append({"model": "model1", "explanation": exp1})
        
        if llm_model2 is not None:
            exp2 = generate_explanation(prediction, model_num=2)
            explanations.append({"model": "model2", "explanation": exp2})
        
        return jsonify({
            "predictions": prediction,
            "explanations": explanations
        })
        
    except Exception as e:
        return jsonify({"error": f"Error in pipeline: {str(e)}"}), 500


# Legacy endpoints for compatibility with existing frontend
@app.route('/api/analyze-image', methods=['POST'])
def analyze_image():
    """
    Legacy endpoint - Analyze image with CNN
    Accepts either:
    - JSON with 'imageUrl' (string)
    - multipart/form-data with 'file'
    Returns: Predictions in format expected by frontend
    """
    try:
        image = None
        
        # Check if JSON request with imageUrl
        if request.is_json:
            data = request.json
            image_url = data.get('imageUrl')
            if image_url:
                # Check if it's a base64 data URL
                if image_url.startswith('data:image'):
                    # Extract base64 data from data URL
                    import base64
                    print(f"[DEBUG] Processing base64 image (length: {len(image_url)})")
                    base64_data = image_url.split(',')[1]
                    image_data = base64.b64decode(base64_data)
                    image = Image.open(io.BytesIO(image_data)).convert('RGB')
                    print(f"[DEBUG] Image loaded: size={image.size}, mode={image.mode}")
                else:
                    # Download image from HTTP URL with proper headers
                    print(f"[DEBUG] Downloading image from URL: {image_url[:100]}...")
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                    }
                    response = requests.get(image_url, headers=headers, timeout=10)
                    response.raise_for_status()
                    image = Image.open(io.BytesIO(response.content)).convert('RGB')
                    print(f"[DEBUG] Image loaded: size={image.size}, mode={image.mode}")
        # Check if file upload
        elif 'file' in request.files:
            file = request.files['file']
            image = Image.open(io.BytesIO(file.read())).convert('RGB')
        else:
            return jsonify({"error": "No image provided. Send 'imageUrl' in JSON or 'file' in form-data"}), 400
        
        if image is None:
            return jsonify({"error": "Could not load image"}), 400
        
        # Get predictions from both models
        print("[DEBUG] Running scratch model prediction...")
        scratch_pred = predict_cnn(image, "scratch", top_k=3)
        print(f"[DEBUG] Scratch predictions: {scratch_pred['style_top']}")

        if cnn_model_finetuned:
            print("[DEBUG] Running finetuned model prediction...")
            finetuned_pred = predict_cnn(image, "finetuned", top_k=3)
            print(f"[DEBUG] Finetuned predictions: {finetuned_pred['style_top']}")
        else:
            finetuned_pred = None
        
        # Use finetuned model if available (better accuracy), otherwise scratch
        pred = finetuned_pred if finetuned_pred else scratch_pred
        
        # Return top-3 predictions for each category
        return jsonify({
            "artist": pred.get('artist_top', [{"label": pred['artist'], "confidence": int(pred['artist_confidence'] * 100)}]),
            "style": pred.get('style_top', [{"label": pred['style'], "confidence": int(pred['style_confidence'] * 100)}]),
            "genre": pred.get('genre_top', [{"label": pred['genre'], "confidence": int(pred['genre_confidence'] * 100)}]),
            "scratch": scratch_pred,
            "finetuned": finetuned_pred,
            "model_used": "finetuned" if finetuned_pred else "scratch"
        })
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"Error processing image: {str(e)}"}), 500


@app.route('/api/generate-llm', methods=['POST'])
def generate_llm():
    """
    Legacy endpoint - Generate LLM explanation
    Expects: JSON with 'model' (scratch/distilgpt2/hosted) and optionally prediction data
    Uses the most recent prediction if available, or requires prediction data
    """
    if llm_model1 is None:
        return jsonify({"error": "LLM models not loaded"}), 503
    
    data = request.json or {}
    model = data.get('model', 'distilgpt2')
    
    # Map frontend model names to backend model numbers
    model_map = {
        'scratch': 1,
        'distilgpt2': 2,  # Model2 is GPT-2 Medium (355M)
        'hosted': 3  # Model3 is Groq Llama 3.2 1B
    }
    
    model_num = model_map.get(model, 1)
    
    # Try to get prediction data from request, or use stored prediction
    # For now, we'll need prediction data in the request
    prediction = {
        'artist': data.get('artist', 'Unknown'),
        'artist_confidence': data.get('artist_confidence', 0.0),
        'style': data.get('style', 'Unknown'),
        'style_confidence': data.get('style_confidence', 0.0),
        'genre': data.get('genre', 'Unknown'),
        'genre_confidence': data.get('genre_confidence', 0.0)
    }
    
    # If we have top predictions from previous call, use those
    if 'predictions' in data:
        pred_data = data['predictions']
        if isinstance(pred_data, dict):
            # Extract top prediction
            if 'artist' in pred_data and len(pred_data['artist']) > 0:
                prediction['artist'] = pred_data['artist'][0].get('label', 'Unknown')
                prediction['artist_confidence'] = pred_data['artist'][0].get('confidence', 0) / 100.0
            if 'style' in pred_data and len(pred_data['style']) > 0:
                prediction['style'] = pred_data['style'][0].get('label', 'Unknown')
                prediction['style_confidence'] = pred_data['style'][0].get('confidence', 0) / 100.0
            if 'genre' in pred_data and len(pred_data['genre']) > 0:
                prediction['genre'] = pred_data['genre'][0].get('label', 'Unknown')
                prediction['genre_confidence'] = pred_data['genre'][0].get('confidence', 0) / 100.0
    
    try:
        explanation = generate_explanation(prediction, model_num=model_num)
        return jsonify({"output": explanation})
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"Error generating explanation: {str(e)}"}), 500


@app.route('/api/generate-text', methods=['POST'])
def generate_text_legacy():
    """
    Generate text from LLM models (for Model Arena)
    Expects: JSON with 'prompt' and optional 'model' (hosted/finetuned/scratch)
    Returns: { text: "..." }
    """
    if llm_model1 is None:
        return jsonify({"error": "LLM models not loaded"}), 503

    data = request.json
    user_prompt = data.get('prompt', '')
    model = data.get('model', 'scratch')

    # Map frontend model names to backend model numbers
    model_num = 1  # Default to model1 (from scratch)
    if model == 'finetuned' and llm_model2 is not None:
        model_num = 2
    elif model == 'hosted':
        model_num = 3  # Model 3 is Groq Llama 3.2 1B

    try:
        # Handle Model 3 (Groq API)
        if model_num == 3:
            if llm_model3_client is None:
                return jsonify({"error": "Model 3 (Groq) not available"}), 503

            try:
                completion = llm_model3_client.chat.completions.create(
                    model=GROQ_MODEL,
                    messages=[{"role": "user", "content": user_prompt}],
                    max_tokens=120,
                    temperature=0.8,
                    top_p=0.95
                )
                text = completion.choices[0].message.content.strip()
                print(f"\n[MODEL ARENA - hosted/groq] OUTPUT: {text[:200]}...\n")

                # Clean up conversational markers
                text = re.sub(r'^(Assistant|User|Human|Expert|Response):\s*', '', text, flags=re.IGNORECASE)

                return jsonify({"text": text})
            except Exception as e:
                print(f"[MODEL 3 - GROQ] Error: {e}")
                return jsonify({"error": f"Groq API error: {str(e)}"}), 500

        # Handle Model 1 and 2 (PyTorch)
        tokens = tokenizer(user_prompt, return_tensors='pt')['input_ids'].to(device)

        with torch.no_grad():
            if model_num == 1:
                # Model 1 - custom architecture
                if hasattr(llm_model1, 'generate') and callable(getattr(llm_model1, 'generate')):
                    output = llm_model1.generate(
                        tokens,
                        max_new_tokens=120,
                        temperature=0.9,
                        top_k=50,
                        top_p=0.95
                    )
                else:
                    output = llm_model1.generate(
                        tokens,
                        max_new_tokens=120,
                        temperature=0.9,
                        do_sample=True,
                        top_k=50,
                        top_p=0.95,
                        repetition_penalty=1.1,
                        pad_token_id=tokenizer.eos_token_id
                    )
            else:
                # Model 2 - fine-tuned GPT-2
                output = llm_model2.generate(
                    tokens,
                    max_new_tokens=120,
                    temperature=0.85,
                    do_sample=True,
                    top_k=50,
                    top_p=0.95,
                    repetition_penalty=1.15,
                    pad_token_id=tokenizer.eos_token_id
                )

        response = tokenizer.decode(output[0], skip_special_tokens=True)
        text = response[len(user_prompt):].strip()

        # Log raw output for debugging
        print(f"\n[MODEL ARENA - {model}] RAW OUTPUT: {text[:200]}...\n")

        # Clean up conversational markers
        text = re.sub(r'^(Assistant|User|Human|Expert|Response):\s*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\b(Assistant|User|Human|Expert):\s*', '', text, flags=re.IGNORECASE)

        # Truncate at conversational markers
        conv_match = re.search(r'\n\s*(Assistant|User|Human|Expert):', text, flags=re.IGNORECASE)
        if conv_match:
            text = text[:conv_match.start()].strip()

        # Check if output is garbage or hallucinating
        word_count = len(text.split())
        number_count = len(re.findall(r'\d+\.?\d*%?', text))

        # ALLOW CNN explanation terms - they're valid for this use case!
        # Only detect severe gibberish (repetitive loops, empty output)
        is_repetitive = bool(re.search(r'(.{10,})\1{3,}', text))  # Same phrase repeated 3+ times

        # If garbage, severely repetitive, EXTREMELY number-heavy (>60%), or very short, use fallback
        should_use_fallback = (
            word_count == 0 or
            (word_count > 0 and number_count / word_count > 0.60) or
            len(text) < 15 or
            is_repetitive
        )

        if should_use_fallback:
            print(f"[FALLBACK] Arena - words: {word_count}, numbers: {number_count}, len: {len(text)}, repetitive: {is_repetitive}")

        if should_use_fallback:
            # Generate a relevant fallback based on the prompt
            if 'art' in user_prompt.lower():
                text = "Art is a diverse range of human creative expression that encompasses visual, auditory, and performance forms. It serves as a medium for conveying emotions, ideas, and cultural narratives throughout history."
            elif 'renaissance' in user_prompt.lower():
                text = "The Renaissance was a cultural rebirth in Europe from the 14th to 17th century, characterized by renewed interest in classical art, humanism, and scientific discovery. Artists like Leonardo da Vinci and Michelangelo created masterpieces that emphasized realism and human emotion."
            elif 'consciousness' in user_prompt.lower():
                text = "Consciousness in art refers to the awareness and intentionality behind creative expression. Artists channel their conscious and subconscious mind to create works that reflect human experience and perception."
            else:
                text = f"Art and creativity have been fundamental to human expression throughout history. The question '{user_prompt}' touches on deep aesthetic and philosophical themes that have inspired artists, scholars, and thinkers across centuries."

        # Ensure complete sentences
        for end in ['. ', '! ', '? ']:
            last_idx = text.rfind(end)
            if last_idx > 30:
                text = text[:last_idx+1].strip()
                break

        # Grammar fixes: ensure proper capitalization
        if text and len(text) > 0:
            # Capitalize first letter
            text = text[0].upper() + text[1:] if len(text) > 1 else text.upper()

            # Fix common spacing issues
            text = re.sub(r'\s+([.,!?])', r'\1', text)  # Remove space before punctuation
            text = re.sub(r'([.,!?])([A-Za-z])', r'\1 \2', text)  # Add space after punctuation

            # Ensure sentences start with capital letters
            text = re.sub(r'(\. )([a-z])', lambda m: m.group(1) + m.group(2).upper(), text)
            text = re.sub(r'(\! )([a-z])', lambda m: m.group(1) + m.group(2).upper(), text)
            text = re.sub(r'(\? )([a-z])', lambda m: m.group(1) + m.group(2).upper(), text)

            # Fix multiple spaces
            text = re.sub(r'\s+', ' ', text)

            # Ensure ends with punctuation
            if text and text[-1] not in '.!?':
                text += '.'

        return jsonify({"text": text})

    except Exception as e:
        # If generation fails, return a safe fallback
        return jsonify({"text": f"Art encompasses diverse creative expressions that reflect human culture, emotion, and ideas across time and medium."})


if __name__ == '__main__':
    app.run(debug=True, port=5000, host='0.0.0.0', use_reloader=False)
